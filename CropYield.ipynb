{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark packages\n",
    "from pyspark import sql, SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize spark session\n",
    "spark_session = sql.SparkSession.builder.appName(\"HDFS\").getOrCreate()\n",
    "spark_context = SparkContext.getOrCreate(SparkConf().setAppName(\"HDFS\"))\n",
    "logs = spark_context.setLogLevel(\"ERROR\")\n",
    "print(\"Spark session initialize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection parameters for spark to Amazon S3\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAUGQ637RYKQTW47FH\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark_session._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-west-1.amazonaws.com\")\n",
    "print(\"Connection to S3 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to load and display dataset from S3 using spark session object\n",
    "dataset = spark_session.read.csv(r\"C:\\Users\\sai ganesh\\OneDrive\\Desktop\\project\\yield_df.csv\", inferSchema=True, header=True)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing dataset with details like count, mean, standard deviation of each dataset attributes\n",
    "dataset.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing distribution of numerical data\n",
    "dataset.toPandas().hist(figsize=(10, 8))\n",
    "plt.title(\"Histogram distribution of dataset values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#graph of different countries found in dataset for making crop yield\n",
    "from pyspark.sql import functions\n",
    "areas = dataset.select('Area').filter(functions.col('Area').isNotNull()).toPandas().values.ravel()\n",
    "names, count = np.unique(areas, return_counts = True)\n",
    "height = count\n",
    "bars = names\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.figure(figsize = (14, 3)) \n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.xlabel(\"Different Area Graph for Crop Yield\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing graph of different crops found in dataset\n",
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(spark_session)\n",
    "#converting spark dataframe to spark sql query\n",
    "dataset.registerTempTable(\"crop\")\n",
    "df = sql.sql(\"SELECT Item from crop\")\n",
    "df = df.toPandas()\n",
    "unique, count = np.unique(df['Item'], return_counts=True)\n",
    "values = []\n",
    "for i in range(len(unique)):\n",
    "    values.append([unique[i], count[i]])\n",
    "values = pd.DataFrame(values, columns = ['Crop', 'Count'])   \n",
    "plt.figure(figsize=(8,3))\n",
    "sns.barplot(x='Crop',y='Count', data=values)\n",
    "plt.title('Most Common Crop Yield by Different Countries')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to visualize different yield of crop by different countries\n",
    "df = sql.sql(\"SELECT Area, Item, yield from crop\")\n",
    "df = df.toPandas()\n",
    "data = df.groupby(['Item', 'Area'])['yield'].sum().sort_values(ascending=False).nlargest(30).reset_index()\n",
    "sns.catplot(x=\"Item\", y=\"yield\", hue='Area', data=data, kind='point')\n",
    "plt.title(\"Crop Yield Graphs of Different Countries\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph of Top 20 highest average rainfall area wise \n",
    "df = sql.sql(\"SELECT Area, average_rain_fall_mm_per_year from crop\")\n",
    "df = df.toPandas()\n",
    "df = df.groupby('Area')['average_rain_fall_mm_per_year'].mean().sort_values(ascending=False).nlargest(20).reset_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(df['Area'], df['average_rain_fall_mm_per_year'])\n",
    "plt.title(\"Top 20 Area Wise Average Rainfall Graph\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph of Top 20 highest area wise pesticides consumption \n",
    "df = sql.sql(\"SELECT Area, pesticides_tonnes from crop\")\n",
    "df = df.toPandas()\n",
    "df = df.groupby('Area')['pesticides_tonnes'].mean().sort_values(ascending=False).nlargest(20).reset_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(df['Area'], df['pesticides_tonnes'])\n",
    "plt.title(\"Top 20 Area Wise Average Pesticides Consumption Graph\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test where application using 80% dataset for training and 20% for testing\n",
    "#extracting train and test features from dataset\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CropYieldPrediction\").getOrCreate()\n",
    "#extracing data for selected country and crop and in above line we are getting all maize yield from India\n",
    "df = spark.sql(\"SELECT * from crop where Area='India' and Item='Potatoes'\")#==============\n",
    "#converting Area and Item column from string to numeric vector\n",
    "indexer = StringIndexer(inputCol=\"Area\", outputCol=\"AreaEncode\")\n",
    "encoder = indexer.fit(df)\n",
    "df = encoder.transform(df)\n",
    "#converting Area and Item column from string to numeric vector\n",
    "indexer = StringIndexer(inputCol=\"Item\", outputCol=\"ItemEncode\")\n",
    "encoder = indexer.fit(df)\n",
    "df = encoder.transform(df)\n",
    "#giving required features for training to select\n",
    "requiredColumns = ['Year', 'yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes','avg_temp', 'AreaEncode', 'ItemEncode']\n",
    "vec_assembler = VectorAssembler(inputCols=requiredColumns, outputCol='train',handleInvalid=\"skip\")\n",
    "transformed = vec_assembler.transform(df)\n",
    "indexer = StringIndexer(inputCol=\"yield\",outputCol=\"predict\",handleInvalid=\"skip\")\n",
    "transformed = indexer.fit(transformed).transform(transformed)\n",
    "#normalizing extracted crop features\n",
    "scaler = MinMaxScaler(inputCol=\"train\", outputCol=\"scaled_train\")\n",
    "transformed = scaler.fit(transformed).transform(transformed)\n",
    "#splitting dataset into train and test\n",
    "(X_train, X_test) = transformed.randomSplit([0.8, 0.2])\n",
    "print(\"80% dataset for training : \"+str(X_train.count()))\n",
    "print(\"20% dataset for testing  : \"+str(X_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler, StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"CropYieldPrediction\").getOrCreate()\n",
    "\n",
    "# Load and preprocess data (similar to your previous code)\n",
    "# ... (Your code for loading and preprocessing data goes here) ...\n",
    "\n",
    "# Define features for training\n",
    "requiredColumns = ['Year', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'AreaEncode', 'ItemEncode']\n",
    "vec_assembler = VectorAssembler(inputCols=requiredColumns, outputCol='features', handleInvalid=\"skip\")\n",
    "transformed = vec_assembler.transform(df)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train, test = transformed.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define models\n",
    "lr = LinearRegression(featuresCol='features', labelCol='yield')\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='yield')\n",
    "\n",
    "\n",
    "# Train models\n",
    "lr_model = lr.fit(train)\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test)\n",
    "dt_predictions = dt_model.transform(test)\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "evaluator = RegressionEvaluator(labelCol=\"yield\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "lr_rmse = evaluator.evaluate(lr_predictions)\n",
    "dt_rmse = evaluator.evaluate(dt_predictions)\n",
    "\n",
    "\n",
    "# Print RMSE values\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Decision Tree RMSE:\", dt_rmse)\n",
    "\n",
    "\n",
    "# Select the best model based on RMSE\n",
    "best_model = min([(lr_rmse, 'Linear Regression'), (dt_rmse, 'Decision Tree')], key=lambda x: x[0])[1]\n",
    "print(\"Best Model:\", best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true and predicted yield values for Decision Tree and Linear Regression\n",
    "true_yield_dt = dt_predictions.select(\"yield\").collect()\n",
    "pred_yield_dt = dt_predictions.select(\"prediction\").collect()\n",
    "\n",
    "true_yield_lr = lr_predictions.select(\"yield\").collect()\n",
    "pred_yield_lr = lr_predictions.select(\"prediction\").collect()\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_yield_dt = np.array([row['yield'] for row in true_yield_dt])\n",
    "pred_yield_dt = np.array([row['prediction'] for row in pred_yield_dt])\n",
    "\n",
    "true_yield_lr = np.array([row['yield'] for row in true_yield_lr])\n",
    "pred_yield_lr = np.array([row['prediction'] for row in pred_yield_lr])\n",
    "\n",
    "# Print first 20 values for Decision Tree\n",
    "print(\"Decision Tree:\")\n",
    "for i in range(20):\n",
    "    print(f\"True Yield = {true_yield_dt[i]}, Predicted Yield = {pred_yield_dt[i]}\")\n",
    "\n",
    "# Print first 20 values for Linear Regression\n",
    "print(\"Linear Regression:\")\n",
    "for i in range(20):\n",
    "    print(f\"True Yield = {true_yield_lr[i]}, Predicted Yield = {pred_yield_lr[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the true_yield_dt and pred_yield_dt arrays\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "\n",
    "# Plot the true yield\n",
    "plt.plot(true_yield_dt, color='red', label='Original Crop Yield')\n",
    "\n",
    "# Plot the predicted yield\n",
    "plt.plot(pred_yield_dt, color='green', label='Decision Tree Crop Yield')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.title('Decision Tree True & Predicted Crop Yield Graph')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the true_yield_lr and pred_yield_lr arrays \n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "\n",
    "# Plot the true yield\n",
    "plt.plot(true_yield_lr, color='red', label='Original Crop Yield') \n",
    "\n",
    "# Plot the predicted yield\n",
    "plt.plot(pred_yield_lr, color='green', label='Linear Regression Crop Yield')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.title('Linear Regression True & Predicted Crop Yield Graph')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data for the bar chart\n",
    "algorithms = ['Decision Tree RMSE', 'Linear Regression RMSE']\n",
    "rmse_values = [dt_rmse, lr_rmse]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(6, 4))  # Adjust figure size if needed\n",
    "plt.bar(algorithms, rmse_values)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Algorithm Names')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Comparison Graph')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Your existing code for training and prediction) ...\n",
    "\n",
    "# Get true and predicted yield values\n",
    "true_yield_dt = dt_predictions.select(\"yield\").collect()\n",
    "pred_yield_dt = dt_predictions.select(\"prediction\").collect()\n",
    "\n",
    "true_yield_lr = lr_predictions.select(\"yield\").collect()\n",
    "pred_yield_lr = lr_predictions.select(\"prediction\").collect()\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_yield_dt = np.array([row['yield'] for row in true_yield_dt])\n",
    "pred_yield_dt = np.array([row['prediction'] for row in pred_yield_dt])\n",
    "\n",
    "true_yield_lr = np.array([row['yield'] for row in true_yield_lr])\n",
    "pred_yield_lr = np.array([row['prediction'] for row in pred_yield_lr])\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(true_yield_dt, pred_yield_dt, color='blue', alpha=0.5)\n",
    "plt.xlabel(\"True Yield (Decision Tree)\")\n",
    "plt.ylabel(\"Predicted Yield (Decision Tree)\")\n",
    "plt.title(\"Decision Tree: True vs. Predicted Yield\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(true_yield_lr, pred_yield_lr, color='red', alpha=0.5)\n",
    "# plt.xlabel(\"True Yield (Linear Regression)\")\n",
    "# plt.ylabel(\"Predicted Yield (Linear Regression)\")\n",
    "# plt.title(\"Linear Regression: True vs. Predicted Yield\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "\n",
    "#creating object of decision tree algorithm\n",
    "dt = DecisionTreeRegressor(featuresCol=\"scaled_train\", labelCol = 'predict')\n",
    "#training DT on selected train data\n",
    "dt_model = dt.fit(X_train)\n",
    "#perform prediction on test data\n",
    "predict = dt_model.transform(X_test)\n",
    "#collect original crop yield\n",
    "true = predict.select('predict').collect()\n",
    "#collect predicted crop yield\n",
    "pred = predict.select('prediction').collect()\n",
    "#calculate decision tree performance using RMSE meric \n",
    "evaluator = RegressionEvaluator(labelCol=\"predict\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_error = evaluator.evaluate(predict)\n",
    "print(\"Decision Tree RMSE = \"+str(rmse_error)+\"\\n\")\n",
    "#plot graph of true and predicted crop yield\n",
    "trueYield = [true[i]['predict'] * 100 for i in range(len(true))]\n",
    "predictedYield = [pred[i]['prediction'] * 100 for i in range(len(pred))]\n",
    "\n",
    "plt.plot(trueYield, color='red', label='Original Crop Yield')\n",
    "plt.plot(predictedYield, color='green', label='Decision Tree Crop Yield')\n",
    "plt.title('Decision Tree True & Predicted Crop Yield Graph')\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "#training linear regression on train features of crop yield dataset\n",
    "lr = LinearRegression(featuresCol=\"scaled_train\", labelCol = 'predict')\n",
    "lr_model = lr.fit(X_train)\n",
    "predict = lr_model.transform(X_test)\n",
    "#collect original crop yield\n",
    "true = predict.select(['predict']).collect()\n",
    "#collect predicted crop yield\n",
    "pred = predict.select(['prediction']).collect()\n",
    "#calculate linear regression performance using RMSE meric \n",
    "evaluator = RegressionEvaluator(labelCol=\"predict\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse_error = evaluator.evaluate(predict)\n",
    "print(\"Linear Regression RMSE = \"+str(lr_rmse_error)+\"\\n\")\n",
    "#plot graph of true and predicted crop yield\n",
    "trueYield = []\n",
    "predictedYield = []\n",
    "for i in range(0, 100): \n",
    "    trueYield.append(true[i].predict*100)\n",
    "for i in range(0, 100): \n",
    "    predictedYield.append(pred[i].prediction*100)\n",
    "for i in range(0, 20):\n",
    "    print(\"True Yield = \"+str(trueYield[i])+\" Linear Regression Predicted Yield = \"+str(predictedYield[i]))\n",
    "plt.plot(trueYield, color = 'red', label = 'Original Crop Yield')\n",
    "plt.plot(predictedYield, color = 'green', label = 'Linear Regression Crop Yield')\n",
    "plt.title('Linear Regression True & Predicted Crop Yield Graph')\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Crop Yield')\n",
    "plt.legend()\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE comaprison Graph\n",
    "height = [rmse, lr_rmse_error]\n",
    "bars = ['Decision Tree TMSE', 'Linear Regression RMSE']\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.figure(figsize = (4, 3))\n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.xlabel(\"Algorithm Names\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE Comparison Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
